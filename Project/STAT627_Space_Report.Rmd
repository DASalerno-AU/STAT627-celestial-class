---
title: "STAT-627 Celestial Classification"
author: "Max Calzada, Josiah Gottfried, Sanghyeob Ko, Domingo Salerno"
date: '2022-12-09'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
pacman::p_load(tidyverse, readxl, XML, jsonlite, tidymodels, infer, reshape2, pwr, janitor, lubridate, skimr, car, vip, caret, MASS, ISLR, glmnet, randomForest, gbm, rpart, boot, broom, gridExtra, ROCR, reshape2, pls, nnet, deepnet, klaR, data.table, tm, SnowballC, wordcloud, leaps, tree, bootstrap, leaps, tree, class)
```

# Introduction

For our project, our central question was "Which statistical model would most optimally classify whether a celestial body is a star, quasar, or galaxy."

To answer this question we built the following models:

- KNN
  - KNN with PCA
- LDA/QDA
  - LDA/QDA with PCA
- Logistic Regression
  - Logistic Regression with PCA
- Trees/Random Forests
  - Trees/Random Forests with PCA

## Data source

The data consists of 100,000 observations of celestial objects by the SDSS (Sloan Digital Sky Survey). Every observation is described by 17 feature columns and 1 class column which identifies it to be either a star, galaxy or quasar.
We dropped the columns irrelevant to the classification question, leaving us with six predictor variables. The variables "u", "r", "g", "i", and "z" measure the light reaching the telescope at different wavelengths. The "redshift" variable is the increase in wavelength due to the distance that the light travels.

<https://www.kaggle.com/datasets/fedesoriano/stellar-classification-dataset-sdss17>

# Load data

```{r}
Space_DF <- read.csv("star_classification.csv") # Upload data
Space_DF <- Space_DF[, c("class", "u", "r", "g", "i", "z", "redshift")]

# Cleaning
Space_DF <- subset(Space_DF, u != -9999) # Remove point with NAs
Space_DF$class <- as.factor(Space_DF$class) # Convert class to a factor

# Display
head(Space_DF)

```

```{r}
# Create indicator variables for logistic regression
Space_DF$galaxy <- ifelse(Space_DF$class == "GALAXY", 1, 0)
Space_DF$star <- ifelse(Space_DF$class == "STAR", 1, 0)
Space_DF$quasar <- ifelse(Space_DF$class == "QSO", 1, 0)
```

# Data exploration

Here we visually explore the relationships between some of the predictors and the class variable.

```{r}
plot(redshift ~ g, data = Space_DF, col = class)
legend("bottomright", legend = levels(Space_DF$class), col = 1:3, pch = 15)
plot(u ~ z, data = Space_DF, col = class)
legend("bottomright", legend = levels(Space_DF$class), col = 1:3, pch = 15)
plot(r ~ i, data = Space_DF, col = class)
legend("bottomright", legend = levels(Space_DF$class), col = 1:3, pch = 15)
```

The plots suggest that redshift is an important predictor of class. They show some potential multicollinearity, so we will also run a principle components analysis.\
We also need to test normality assumptions for discriminant analysis.

```{r}
norm_plots <- function(variable){
  
  # Histogram with normal curve
  hist(variable, freq = F, main = paste(deparse(substitute(variable))))
  curve(dnorm(x, mean = mean(variable), sd = sd(variable)), add = TRUE, col = "blue")
  
  # Theoretical quantiles plot (with the first 5000 points to save computing power)
  qqnorm(variable[1:5000], main = paste(deparse(substitute(variable))))
  qqline(variable[1:5000])
  
}

par(mfrow = c(1,2))
norm_plots(Space_DF$u)
norm_plots(Space_DF$r)
norm_plots(Space_DF$g)
norm_plots(Space_DF$i)
norm_plots(Space_DF$z)
norm_plots(Space_DF$redshift)
```

All variables look reasonably normal except redshift.

```{r}
# Since the data contain negative values, let's try signed square root
signsqrt <- function(x){
  sign(x) * abs(x)^(1/2)
}

# Transform redshift
Space_DF$trans.redshift <- signsqrt(Space_DF$redshift)
norm_plots(Space_DF$trans.redshift)
```

With the signed square root transformation, the data look closer to normal.

# Split data into training and testing sets

To test our results, we will use both validation set and 10-fold cross-validation.

```{r}
set.seed(1)
n.S <- nrow(Space_DF) 
Z.S <- sample(n.S, n.S*0.9) # We’ll split data at random 

S.Train <- Space_DF[Z.S,] # training data
S.Test <- Space_DF[-Z.S,] # testing data
X.S.Train <- S.Train[, c("u", "r", "g", "i", "z", "trans.redshift")]
X.S.Test <- S.Test[, c("u", "r", "g", "i", "z", "trans.redshift")]
Y.S.Train <- Space_DF$class[Z.S]
Y.S.Test <- Space_DF$class[-Z.S]
```

```{r}
# Set up folds for 10-fold validation
set.seed(1)
fold <- sample(rep(1:10, 10000), n.S)
```

```{r}
# Initialize matrix for cross validation results
CV.results <- as.data.frame(matrix(nrow = 12, ncol = 2))
rownames(CV.results) <- c("KNN", "LDA", "QDA", "Logistic", "Tree", "RF",
                          "KNN.PC", "LDA.PC", "QDA.PC", "Logistic.PC", "Tree.PC", "RF.PC")
colnames(CV.results) <- c("ValSet", "KFold")
```

# Calculate principal components

We will try each of our models using the original data against models using the first three principal components. In addition to addressing multicollinearity issues, we hope that the principal components will allow us to cut the size of the data in half.

```{r}
X.pca <-  model.matrix(class ~ u + r + g + i + z + trans.redshift, data = Space_DF)[,-1]
pc <-  prcomp(X.pca, scale = TRUE)
summary(pc)
pc_df <- cbind(pc$x, Space_DF[c("class", "galaxy", "star", "quasar")])
head(pc_df)

# Training set / Testing set of PC data
n.PC <- length(pc_df$class)
S.Train.PC <- pc_df[Z.S,]
S.Test.PC <- pc_df[-Z.S,]
```

The first three principal components explain 98% of the variance.

```{r}
norm_plots(pc_df$PC1)
norm_plots(pc_df$PC2)
norm_plots(pc_df$PC3)
```

The principal components appear to follow normal distributions, satisfying the assumption for discriminant analysis.

# Models

## KNN


```{r}
set.seed(1)
knn.S = knn(X.S.Train, X.S.Test, Y.S.Train, 3) # K = 3?
table(Y.S.Test, knn.S)
```

```{r}
mean(Y.S.Test == knn.S )
```

```{r}
# Tuning
class.rate = rep(0,10)

set.seed(1)
for (K in 1:10) {
  knn.S = knn(X.S.Train, X.S.Test, Y.S.Train, K)
  class.rate[K] = mean(Y.S.Test == knn.S)
}

class.rate
```

```{r}
which.max(class.rate)
CV.results["KNN", "ValSet"] <- max(class.rate)
```

```{r}
# K20 <- as.vector(1:20)
# plot(K,class.rate)
plot(class.rate, type = "l", xlab = "K")
```


## LDA/QDA
1. Data Analysis 1 (LDA)
1-1. Linear Discriminant Analysis (LDA)
```{r}
# library(MASS)
LDA <- lda(class ~ u + g + r + i + z + trans.redshift, data = Space_DF) # The main command for LDA
LDA
# Prior probabilities of groups: These are sample proportions of the 3 groups, from our data
# Group means: Multivariate group means are computed within each group
# Coefficients of linear discriminants: These printed coefficients determine the Fisher's linear discriminants LD1, LD2. The first one is a linear function that achieves the maximal separation of our four groups. LD2 is a linear function, orthogonal to LD1, that achieves the maximal separation among all linear functions orthogonal to LD1, etc. These functions are linear combinations of our linear discriminant functions. Their derivation is based on Linear Algebra.
# Here, LD1 captures 87.93% of differences between the groups, LD2 adds 12.07% to that.
```

1-2. LDA Cross-validation (CV: LOOCV)
```{r}
# Cross-validation
# Option CV=TRUE is used for “leave one out” cross-validation; for each sampling unit, it gives its class assignment without the current observation. This is a method of estimating the testing classifications rate instead of the training rate.
LDA.fit <- lda(class ~ u + g + r + i + z + trans.redshift, CV = TRUE, data = Space_DF)
table(Space_DF$class, LDA.fit$class) # The main diagonal shows correctly classified counts.
mean(Space_DF$class == LDA.fit$class) # 94.54%: Correct classification rate = proportion of correctly classified counts.
```

1-3. LDA Cross-validation (CV: Cross-validation, training:testing = 9:1)
```{r}
LDA.training <- lda(class ~ u + g + r + i + z + trans.redshift, data = S.Train) # Only used the training set
LDA.pred <- predict(LDA.training, newdata = S.Test) # Prediction by using the testing set
table(S.Test$class, LDA.pred$class) # The main diagonal shows correctly classified counts.
CV.results["LDA", "ValSet"] <- mean(S.Test$class == LDA.pred$class) 
CV.results["LDA", "ValSet"] # 94.35%: Correct classification rate = proportion of correctly classified counts.
```

1-4. LDA Cross-validation (CV: k-fold 10)
```{r}
pred_rate <- NULL
for(i in 1:10){
  train_kf <- Space_DF[fold != i,]
  test_kf <- Space_DF[fold == i,]
  lda_kf <- lda(class ~ u + g + r + i + z + trans.redshift, data = train_kf )
  lda_kf_pred <- predict(lda_kf, test_kf)
  pred_rate[i] <- mean(test_kf$class == lda_kf_pred$class)
}
CV.results["LDA", "KFold"] <- mean(pred_rate) 
CV.results["LDA", "KFold"] # 94.53% for K-fold 10 validation of LDA
```


2. Data Analysis 2 QDA
2-1. Quadratic Discriminant Analysis (QDA)
```{r}
QDA <- qda(class ~ u + g + r + i + z + trans.redshift, data = Space_DF) # The main command for QDA
QDA
# Difference between LDA and QDA: As for the assumption of QDA, the observation of each class is drawn from a normal distribution, and it is the same as LDA. But the assumption that each class has its own covariance matrix is different from LDA.
```

2-2. QDA Cross-validation (CV: LOOCV)
```{r}
# Quadratic Discriminant Analysis
QDA.fit <- qda(class ~ u + g + r + i + z + trans.redshift, CV = TRUE, data = Space_DF)
table(Space_DF$class, QDA.fit$class) # The main diagonal shows correctly classified counts.
mean(Space_DF$class == QDA.fit$class)
table(is.na(QDA.fit$class)) # There are 7 NAs in the result.
(55788+17428+21555)/(55788+3146+509+1524+17428+4+29+9+21555) # 94.77%: Correct classification rate = proportion of correctly classified counts.
```

2-3. QDA Cross-validation (CV: Cross-validation, training:testing = 9:1)
```{r}
QDA.training <- qda(class ~ u + g + r + i + z + trans.redshift, data = S.Train) # Only used the training set
QDA.pred <- predict(QDA.training, newdata = S.Test) # Prediction by using the testing set
table(S.Test$class, QDA.pred$class) # The main diagonal shows correctly classified counts.
CV.results["QDA", "ValSet"] <- mean(S.Test$class == QDA.pred$class) 
CV.results["QDA", "ValSet"] # 94.91%: Correct classification rate = proportion of correctly classified counts.
```

2-4. QDA Cross-validation (CV: k-fold 10)
```{r}
pred_rate <- NULL
for(i in 1:10){
  train_kf <- Space_DF[fold != i,]
  test_kf <- Space_DF[fold == i,]
  qda_kf <- qda(class ~ u + g + r + i + z + trans.redshift, data = train_kf )
  qda_kf_pred <- predict(qda_kf, test_kf)
  pred_rate[i] <- mean(test_kf$class == qda_kf_pred$class)
}
CV.results["QDA", "KFold"] <- mean(pred_rate) 
CV.results["QDA", "KFold"] # 94.77% for K-fold 10 validation of QDA
```


3. LDA & QDA with PC (PC1, PC2, PC3)
3-1. LDA with PC
```{r}
LDA.PC <- lda(class ~ PC1 + PC2 + PC3, data = pc_df)
LDA.PC 
# LD1 captures 91.08% of differences between the groups, LD2 adds 8.92% to that. The result is better than before. Originally, LD1 captured 87.93% of differences between the groups, LD2 adds 12.07% to that.
```

3-2. LDA with PC Cross-validation (CV: LOOCV)
```{r}
LDA.fit.PC <- lda(class ~ PC1 + PC2 + PC3, CV = TRUE, data = pc_df)
table(pc_df$class, LDA.fit.PC$class) 
mean(pc_df$class == LDA.fit.PC$class) # 82.71%. It is quite lower than original 94.54%, correct classification rate. It seems that the change of redshift may be the reason of the difference.
```

3-3. LDA with PC Cross-validation (CV: Cross-validation, training:testing = 9:1)
```{r}
LDA.training.PC <- lda(class ~ PC1 + PC2 + PC3, data = S.Train.PC)
LDA.pred.PC <- predict(LDA.training.PC, newdata = S.Test.PC)
table(S.Test.PC$class, LDA.pred.PC$class)
CV.results["LDA.PC", "ValSet"] <- mean(S.Test.PC$class == LDA.pred.PC$class) 
CV.results["LDA.PC", "ValSet"] # 82.82%. It is quite lower than original 94.35%, correct classification rate.
```

3-4. LDA with PC Cross-validation (CV: k-fold 10)
```{r}
pred_rate <- NULL
for(i in 1:10){
  train_kf <- pc_df[fold != i,]
  test_kf <- pc_df[fold == i,]
  lda_kf <- lda(class ~ PC1 + PC2 + PC3, data = train_kf )
  lda_kf_pred <- predict(lda_kf, test_kf)
  pred_rate[i] <- mean(test_kf$class == lda_kf_pred$class)
}
CV.results["LDA.PC", "KFold"] <- mean(pred_rate)
CV.results["LDA.PC", "KFold"] # 82.71%, It is quite lower than original 94.53% for K-fold 10 validation of LDA
```

3-5. QDA with PC
```{r}
QDA.PC <- qda(class ~ PC1 + PC2 + PC3, data = pc_df) 
QDA.PC
```

3-6. QDA with PC Cross-validation (CV: LOOCV)
```{r}
QDA.fit.PC <- qda(class ~ PC1 + PC2 + PC3, CV = TRUE, data = pc_df)
table(pc_df$class, QDA.fit.PC$class)
mean(pc_df$class == QDA.fit.PC$class) # 94.55%. It is almost the same as the original 94.77%
```

3-7. QDA with PC Cross-validation (CV: Cross-validation, training:testing = 9:1)
```{r}
QDA.training.PC <- qda(class ~ PC1 + PC2 + PC3, data = S.Train.PC)
QDA.pred.PC <- predict(QDA.training.PC, newdata = S.Test.PC)
table(S.Test.PC$class, QDA.pred.PC$class)
CV.results["QDA.PC", "ValSet"] <- mean(S.Test.PC$class == QDA.pred.PC$class) 
CV.results["QDA.PC", "ValSet"] # 94.47%, it is almost the same as the original 94.91%.
```

3-8. QDA with PC Cross-validation (CV: k-fold 10)
```{r}
pred_rate <- NULL
for(i in 1:10){
  train_kf <- pc_df[fold != i,]
  test_kf <- pc_df[fold == i,]
  qda_kf <- qda(class ~ PC1 + PC2 + PC3, data = train_kf )
  qda_kf_pred <- predict(qda_kf, test_kf)
  pred_rate[i] <- mean(test_kf$class == qda_kf_pred$class)
}
CV.results["QDA.PC", "KFold"] <- mean(pred_rate) 
CV.results["QDA.PC", "KFold"] # 94.55%. It is almost the same as the original 94.77%
```


## Logistic regression

1. Define individual logistic models
```{r}
# Calculate individual logistic models for each celestial object
galaxy.mod <- glm(galaxy ~ u + g + r + i + z + redshift, data = Space_DF, family = binomial)
star.mod <- glm(star ~ u + g + r + i + z + redshift, data = Space_DF, family = binomial)
quasar.mod <- glm(quasar ~ u + g + r + i + z + redshift, data = Space_DF, family = binomial)

summary(galaxy.mod)
summary(star.mod)
summary(quasar.mod)
```

2. Select best subset
```{r}
# criteria
library(leaps)
criteria <- NULL
for(Y in list(Space_DF$galaxy, Space_DF$star, Space_DF$quasar)){
  subsets <- regsubsets(Y ~ u + g + r + i + z + redshift, data = Space_DF)
  criteria <- rbind(criteria,
                    -summary(subsets)$adjr2, # converted to negative so that best is minimum
                    summary(subsets)$bic,
                    abs(summary(subsets)$cp - 1:6)) # absolute difference so that best is minimum
}
criteria <- as.data.frame(criteria)
rownames(criteria) <- c("-adjR2 (galaxy)", "BIC (galaxy)", "|Cp - p| (galaxy)",
                        "-adjR2 (star)", "BIC (star)", "|Cp - p| (star)",
                        "-adjR2 (quasar)", "BIC (quasar)", "|Cp - p| (quasar)")
criteria$best <- apply(criteria, 1, which.min)
criteria
```

```{r}
# stepwise
full <- galaxy.mod
null <- glm(galaxy ~ 1, data = Space_DF, family = binomial)
stats::step(null, scope = list(lower = null, upper = full), direction = "forward", trace = 0)
stats::step(full, scope = list(lower = null, upper = full), direction = "backward", trace = 0)
```
All methods indicate that the best subset includes all predictors

3. Cross-validation
```{r}
cv.logistic <- function(train, test, predictors){
  # This function trains a logistic model for each type of celestial object, then
  # predicts the type of celestial object based on which one is most probable
  
  # Inputs
    # train: A training data set
    # test: A testing data set
    # predictors: The names of predictors to include in the model
  
  # Outputs
  
  formula <- list(as.formula(paste("galaxy ~", paste(predictors, collapse = "+"))),
                  as.formula(paste("star ~", paste(predictors, collapse = "+"))),
                  as.formula(paste("quasar ~", paste(predictors, collapse = "+"))))
  
  mod <- list(galaxy = glm(formula[[1]], data = train, family = binomial),
              star = glm(formula[[2]], data = train, family = binomial),
              quasar = glm(formula[[3]], data = train, family = binomial))
  
  pred <- data.frame(GALAXY = predict(mod$galaxy, newdata = test, type = "response"),
                     STAR = predict(mod$star, newdata = test, type = "response"),
                     QSO = predict(mod$quasar, newdata = test, type = "response"))
  
  pred$class <- apply(pred, 1, function(x) names(which.max(x)))
  
  return(list(predictions = pred,
              class.rate = mean(pred$class == test$class),
              confusion.matrix = table(pred$class, test$class)))
  
}
```

```{r}
# cross-validation
predictors <- c("u", "g", "r", "i", "z", "trans.redshift")

VS <- cv.logistic(S.Train, S.Test, predictors)
CV.results["Logistic", "ValSet"] <- VS$class.rate
VS$confusion.matrix
```

```{r}
# 10-fold
class.rates <- NULL

for(i in 1:10){
  train <- Space_DF[fold != i,]
  test <- Space_DF[fold == i,]
  KF <- suppressWarnings(cv.logistic(train, test, predictors))
  class.rates <- c(class.rates, KF$class.rate)
}

CV.results["Logistic", "KFold"] <- mean(class.rates)
```

4. Logistic regression with principle components
```{r}
# cross-validation
predictors <- c("PC1", "PC2", "PC3")

VS <- cv.logistic(S.Train.PC, S.Test.PC, predictors)
CV.results["Logistic.PC", "ValSet"] <- VS$class.rate
VS$confusion.matrix
```

```{r}
# 10-fold

class.rates <- NULL

for(i in 1:10){
  train <- pc_df[fold != i,]
  test <- pc_df[fold == i,]
  KF <- suppressWarnings(cv.logistic(train, test, predictors))
  class.rates <- c(class.rates, KF$class.rate)
}

CV.results["Logistic.PC", "KFold"] <- mean(class.rates)
```


## Trees/Random Forests

### Single Tree

```{r VS pruned tree}
# train model and plot
tree <- tree(class ~ u + g + r + i + z + redshift, S.Train)
y_hat <- predict(tree, newdata = S.Test, type = 'class')
plot(tree, type = 'uniform'); text(tree)

# show performance on testing data
CV.results["Tree", "ValSet"] <- mean(y_hat == Y.S.Test)
table(y_hat, Y.S.Test)
```

```{r K-fold pruned tree}
# train model and plot
tree <- tree(class ~ u + g + r + i + z + redshift, Space_DF)
tree.cv <- cv.tree(tree, FUN = prune.misclass)
tree.cv.size <- tree.cv$size[which.min(tree.cv$dev)]
tree.pruned <- prune.misclass(tree, best = tree.cv.size)
plot(tree.pruned, type = 'uniform'); text(tree.pruned)

# show performance of model
tree.pruned.s <- summary(tree.pruned)
CV.results["Tree", "KFold"] <- 1 - tree.pruned.s[[7]][1] / tree.pruned.s[[7]][2]
```

```{r VS pruned tree pca}
# train model and plot
tree <- tree(class ~ PC1 + PC2 + PC3, S.Train.PC)
y_hat <- predict(tree, newdata = S.Test.PC, type = 'class')
plot(tree, type = 'uniform'); text(tree)

# show performance on testing data
CV.results["Tree.PC", "ValSet"] <- mean(y_hat == Y.S.Test)
table(y_hat, Y.S.Test)
```

```{r K-fold pruned tree pca}
# train model and plot
tree <- tree(class ~ PC1 + PC2 + PC3, pc_df)
tree.cv <- cv.tree(tree, FUN = prune.misclass)
tree.cv.size <- tree.cv$size[which.min(tree.cv$dev)]
tree.pruned <- prune.misclass(tree, best = tree.cv.size)
plot(tree.pruned, type = 'uniform'); text(tree.pruned)

# show performance of model
tree.pruned.s <- summary(tree.pruned)
CV.results["Tree.PC", "KFold"] <- 1 - tree.pruned.s[[6]][1] / tree.pruned.s[[6]][2]
```

### Random Forests

```{r VS random forest}
# initialize error and tree number variables
rf.err <- 0
ntrees <- 0
for(i in 1:5){
  rf <- randomForest(class ~ u + g + r + i + z + redshift, 
                     data = S.Train, mtry = i)
  rf.ntrees <- which.min(rf$err.rate)
  rf <- randomForest(class ~ u + g + r + i + z + redshift, 
                     data = S.Train, mtry = i, ntree = rf.ntrees)
  y_hat <- predict(rf, newdata = S.Test)
  rf.err[i] <- mean(y_hat != Y.S.Test)
  ntrees[i] <- rf.ntrees
}

plot(rf.err); lines(rf.err)

rf.optimal <- randomForest(class ~ u + g + r + i + z + redshift, 
                           data = S.Train,
                           mtry = which.min(rf.err), 
                           ntrees = ntrees[which.min(rf.err)]
                           )
y_hat <- predict(rf.optimal, newdata = S.Test, type = 'class')
table(y_hat, Y.S.Test)
CV.results["RF", "ValSet"] <- mean(y_hat == Y.S.Test)
```

```{r VS random forest PCA}
# initialize error and tree number variables
rf.err <- 0
ntrees <- 0
for(i in 1:5){
  rf <- randomForest(class ~ PC1 + PC2 + PC3, 
                     data = S.Train.PC, mtry = i)
  rf.ntrees <- which.min(rf$err.rate)
  rf <- randomForest(class ~ PC1 + PC2 + PC3, 
                     data = S.Train.PC, mtry = i, ntree = rf.ntrees)
  y_hat <- predict(rf, newdata = S.Test)
  rf.err[i] <- mean(y_hat != Y.S.Test)
  ntrees[i] <- rf.ntrees
}

plot(rf.err); lines(rf.err)

rf.optimal <- randomForest(class ~ PC1 + PC2 + PC3, 
                           data = S.Train.PC,
                           mtry = which.min(rf.err), 
                           ntrees = ntrees[which.min(rf.err)]
                           )
y_hat <- predict(rf.optimal, newdata = S.Test, type = 'class')
table(y_hat, Y.S.Test)
CV.results["RF.PC", "ValSet"] <- mean(y_hat == Y.S.Test)
```

# Results

The model with the best prediction rate was the random forest model at 97.76% accuracy.
